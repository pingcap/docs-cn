---
title: TiDB TPC-H 性能对比测试报告 - v4.0 对比 v3.0
category: benchmark
---

# TiDB TPC-H 性能对比测试报告 - v4.0 对比 v3.0

## 测试目的

对比 TiDB v4.0 和 v3.0 OLAP 场景下的性能。

因为 TiDB v4.0 中新引入了 [TiFlash](/tiflash/tiflash-overview.md) 组件增强 TiDB HTAP 形态，本文的测试对象如下：

+ v3.0 仅从 TiKV 读取数据。
+ v4.0 仅从 TiKV 读取数据。
+ v4.0 通过智能选择混合读取 TiKV、TiFlash 的数据。

## 测试环境 (AWS EC2)

### 硬件配置

| 服务类型         | EC2 类型     | 实例数 |
|:----------------|:------------|:----|
| PD              | m5.xlarge   |  3  |
| TiDB            | c5.4xlarge  |  2  |
| TiKV & TiFlash  | i3.4xlarge  |  3  |
| TPC-H           | m5.xlarge   |  1  |

### 软件版本

| 服务类型   | 软件版本   |
|:----------|:-----------|
| PD        | 3.0、4.0   |
| TiDB      | 3.0、4.0   |
| TiKV      | 3.0、4.0   |
| TiFlash   | 4.0        |
| tiup-bench | 0.2      |

### 配置参数

#### v3.0

v3.0 的 TiDB 和 TiKV 均为默认参数配置。

##### 全局变量配置

{{< copyable "sql" >}}

```sql
set global tidb_distsql_scan_concurrency = 30;
set global tidb_projection_concurrency = 16;
set global tidb_hashagg_partial_concurrency = 16;
set global tidb_hashagg_final_concurrency = 16;
set global tidb_hash_join_concurrency = 16;
set global tidb_index_lookup_concurrency = 16;
set global tidb_index_lookup_join_concurrency = 16;
```

#### v4.0

v4.0 的 TiDB 为默认参数配置。

##### TiKV 配置

{{< copyable "" >}}

```yaml
readpool.storage.use-unified-pool: false
readpool.coprocessor.use-unified-pool: true
```

##### PD 配置

{{< copyable "" >}}

```yaml
replication.enable-placement-rules: true
```

##### TiFlash 配置

{{< copyable "" >}}

```yaml
logger.level: "info"
learner_config.log-level: "info"
```

##### 全局变量配置

{{< copyable "sql" >}}

```sql
set global tidb_allow_batch_cop = 1;
set global tidb_opt_distinct_agg_push_down = 1;
set global tidb_distsql_scan_concurrency = 30;
set global tidb_projection_concurrency = 16;
set global tidb_hashagg_partial_concurrency = 16;
set global tidb_hashagg_final_concurrency = 16;
set global tidb_hash_join_concurrency = 16;
set global tidb_index_lookup_concurrency = 16;
set global tidb_index_lookup_join_concurrency = 16;
```

### 测试方案

#### 硬件准备

为了避免 TiKV 和 TiFlash 争抢磁盘和 I/O 资源，把 EC2 配置的两个 NVMe SSD 盘分别挂载为 `/data1` 及 `/data2`，把 TiKV 的部署至 `/data1`，TiFlash 部署至 `/data2`。

#### 测试过程

1. 通过 TiUP 部署 TiDB v4.0 和 v3.0。

2. 通过 TiUP 的 bench 工具导入 TPC-H 10 数据。

    * 执行以下命令将数据导入 v3.0：

        {{< copyable "bash" >}}

        ```bash
        tiup bench tpch prepare \
        --host ${tidb_v3_host} --port ${tidb_v3_port} --db tpch_10 \
        --sf 10 \
        --analyze --tidb_build_stats_concurrency 8 --tidb_distsql_scan_concurrency 30
        ```

    * 执行以下命令将数据导入 v4.0：

        {{< copyable "bash" >}}

        ```bash
        tiup bench tpch prepare \
          --host ${tidb_v4_host} --port ${tidb_v4_port} --db tpch_10 --password ${password} \
          --sf 10 \
          --tiflash \
          --analyze --tidb_build_stats_concurrency 8 --tidb_distsql_scan_concurrency 30
        ```

3. 运行 TPC-H 的查询。

    1. 下载 TPC-H 的 SQL 查询文件：

        {{< copyable "" >}}

        ```bash
        git clone https://github.com/pingcap/tidb-bench.git && cd tpch/queries
        ```

    2. 查询并记录耗时。

        * 对于 TiDB v3.0，使用 MySQL 客户端连接到 TiDB，然后执行查询，记录 v3.0 查询耗时。
        * 对于 TiDB v4.0，使用 MySQL 客户端连接到 TiDB，再根据测试的形态，选择其中一种操作：
            * 设置 `set @@session.tidb_isolation_read_engines = 'tikv,tidb';` 后，再执行查询，记录 v4.0 仅从 TiKV 读取数据的查询耗时。
            * 设置 `set @@session.tidb_isolation_read_engines = 'tikv,tiflash,tidb';` 后，再执行查询，记录 v4.0 通过智能选择从 TiKV 和 TiFlash 混合读取数据的查询耗时。

4. 提取整理耗时数据。

### 测试结果

| Query ID |  v3.0  |  v4.0 TiKV Only |  v4.0 TiKV / TiFlash Automatically |
| :-------- | :----------- | :------------ | :-------------- |
| 1       |    7.78s   |      7.45s  |      2.09s    |
| 2       |    3.15s   |      1.71s  |      1.71s    |
| 3       |    6.61s   |      4.10s  |      4.05s    |
| 4       |    2.98s   |      2.56s  |      1.87s    |
| 5       |   20.35s   |      5.71s  |      8.53s    |
| 6       |    4.75s   |      2.44s  |      0.39s    |
| 7       |    7.97s   |      3.72s  |      3.59s    |
| 8       |    5.89s   |      3.22s  |      8.59s    |
| 9       |   34.08s   |     11.87s  |     15.41s    |
| 10      |    4.83s   |      2.75s  |      3.35s    |
| 11      |    3.98s   |      1.60s  |      1.59s    |
| 12      |    5.63s   |      3.40s  |      1.03s    |
| 13      |    5.41s   |      4.56s  |      4.02s    |
| 14      |    5.19s   |      3.10s  |      0.78s    |
| 15      |   10.25s   |      1.82s  |      1.26s    |
| 16      |    2.46s   |      1.51s  |      1.58s    |
| 17      |   23.76s   |     12.38s  |      8.52s    |
| 18      |   17.14s   |     16.38s  |     16.06s    |
| 19      |    5.70s   |      4.59s  |      3.20s    |
| 20      |    4.98s   |      1.89s  |      1.29s    |
| 21      |   11.12s   |      6.23s  |      6.26s    |
| 22      |    4.49s   |      3.05s  |      2.31s    |

![TPC-H](/media/tpch_v4vsv3.png)

以上性能图中蓝色为 v3.0，红色为 v4.0（仅从 TiKV 读），黄色为 v4.0（从 TiKV、TiFlash 智能选取），纵坐标是查询的处理时间。纵坐标越低，表示性能越好。

- v4.0（仅从 TiKV 读取数据），即 TiDB 仅会从 TiKV 中读取数据。将该结果与 v3.0 的结果对比可得知，TiDB、TiKV 升级至 4.0 版本后，TPC-H 性能得到的提升幅度。
- v4.0（从 TiKV、TiFlash 智能选取），即 TiDB 优化器会自动根据代价估算选择是否使用 TiFlash 副本。将该结果与 v3.0 的结果对比可得知，在 v4.0 完整的 HTAP 形态下，TPC-H 性能得到的提升幅度。
