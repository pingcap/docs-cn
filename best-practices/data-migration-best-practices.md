---
title: 数据迁移最佳实践
summary: 了解使用 Data Migration (DM) 进行数据迁移的一些最佳实践。
---

# 数据迁移最佳实践

Data Migration (DM) 是由 PingCAP 开发维护的数据迁移同步工具，主要支持的源数据库类型为各类 MySQL 协议标准的关系型数据库，如 MySQL、Percona MySQL、MariaDB、AWS MYSQL RDS、AWS Aurora 等。DM 主要有以下几个使用场景：

- 单一实例全量和增量迁移
- 将分库分表的各个库表归并到一张总表的全量和增量迁移
- 在“业务数据中台、业务数据实时汇聚”等 DataHUB 场景中，作为数据同步中间件来使用

本文档介绍了如何优雅高效的使用 DM，以及如何规避使用 DM 时的常见误区。

## 性能边界定位

| 参数 | 限制 |
| -------- | :------: |
|  最大同步节点（Work Nodes ） |  1000  |
|  最大同步任务数量 |  600  |
|  最大同步 QPS   |  30k QPS/worker |
|  最大 Binlog 吞吐量  |  20 MB/s/worker |
|   SLA     |  >99.9% |
|   每个 Task 处理的表数量  | 无限制 |

- DM 支持同时管理 1000 个同步节点（Work Node），最大同步任务数量为 600 个。为了保证同步节点的高可用，应预留一部分 Work Node 节点作为备用节点，保证数据同步的高可用。预留已开启同步任务 Work Node 数量的 20% ~ 50%。
- 单机部署 Work Node 数量。在服务器配置较好情况下，要保证每个 Work Node 至少有 2 核 CPU 加 4G 内存的工作资源可以使用，并且应为主机预留  10% ~ 20% 的系统资源。
- 单个同步节点（Work Node ），理论最大同步 QPS 在 30K QPS/worker（不同 Schema 和 workload 会有所差异），处理上游 Binlog 的能力最高为 20MB/。
- 如果将 DM 作为需要长期使用的数据同步中间件，SLA 可以达到 3 个 9 以上。但需要注意 DM 组件的部署架构，参见[xxx](#xxx)。

## 数据迁移前的最佳实践

在所有数据迁移之前，整体方案的设计是至关重要的。尤其以迁移前的一系列方案设计，是整个方案实施好坏的重中之重。下面我们分别从业务侧要点及实施侧要点两个方面讲一下相关的实践经验和适用场景。

### 业务侧要点

为了让压力可以平均分配到多个节点上，在 Schema 设计上，分布式数据库与传统数据库差别很大，既要保证较低的业务迁移成本，又要保证迁移后应用逻辑的正确性。下面就从几个方面来看业务迁移前的最佳实践。

#### Schema 的设计中 AUTO_INCREMENT 对业务的影响

TiDB 的 AUTO_INCREMENT 与 MySQL AUTO_INCREMENT 整体上看是相互兼容的。但因为 TiDB 作为分布式数据库，一般会有多个计算节点（client 端入口），应用数据写入时会将负载均分开，这就导致在有 AUTO_INCREMENT 列的表上，可能出现不连续的自增 ID。详细原理参考 [AUTO_INCREMENT](/auto-increment.md#实现原理)。

如果业务对自增 ID 有强依赖，可以考虑使用 [SEQUENCE 函数](/sql-statements/sql-statement-create-sequence.md#sequence-函数)。

#### 是否使用聚簇索引

TiDB 在建表时可以声明为主键创建聚簇索引或非聚簇索引。下面介绍各方案的优势和劣势。

- 聚簇索引
    [聚簇索引](/clustered-indexes.md#聚簇索引)使用主键作为数据存储的 handle ID（行 ID），在使用主键查询时可以减少一次回表的操作，有效提升查询效能。但如果表有大量数据写入且主键使用 [AUTO_INCREMENT](/auto-increment.md#实现原理)，非常容易造成数据存储的[写入热点问题](/best-practices/high-concurrency-best-practices.md#高并发批量插入场景)，导致集群整体效能不能充分利用，出现单存储节点的性能瓶颈问题。

- 非聚簇索引 + `shard row id bit`
    使用非聚簇索引，再配合表提示 `shard row id bit`，可以在继续使用 AUTO_INCREMENT 的情况下有效避免数据写入热点的产生。但此时使用主键查询数据，因为多了一次回表操作，查询性能将有所影响。

- 聚簇索引 + 外部分布式发号器
    如果想使用聚簇索引，但还希望 ID 是单调递增的，那么可以考虑使用外部分布式发号器，如 Snowflake、Leaf 等，来解决问题。由应用程序产生序列 ID，可以一定程度上保证 ID 的单调递增性，并同时也保留了使用聚簇索引带来的收益。但相对的应用程序需要进行改造。

- 聚簇索引 + AUTO_RANDOM
    此方案是目前分布式数据库既能避免出现写入热点问题，同时又能保留聚簇索引带来的查询收益的方案。整体改造也相对轻量，可以在业务切换使用 TiDB 作为写库时，修改 Schema 属性来达到目的。那么在后续查询时一定要利用 ID 列进行排序，可以使用 AUTO_RANDOM ID 再列左移 5 位来保证查询数据的顺序性。示例：

    ```sql
    CREATE TABLE t (a bigint PRIMARY KEY AUTO_RANDOM, b varchar(255));
    Select  a, a<<5 ,b from t order by a <<5 desc
    ```

下表汇总了各方案的使用场景和优劣势。

| 场景 | 推荐方案 | 优势 | 劣势 |
| :--- | :--- | :--- | :--- |
|  TIDB 未来作为主库使用，并会有大量数据写入。业务逻辑强依赖主键 ID 的连续性。  |  将表建立为非聚簇索引，并设置 SHARD_ROW_ID_BIT。使用 SEQUENCE 作为主键列。   |  可以有效避免数据写入热点，保证业务数据的连续性和单调递增。 | 数据写入的吞吐能力会下降（为保证数据写入连续性）；主键查询性能有所下降。 |
|   TIDB 未来作为主库使用，并会有大量数据写入。业务逻辑强依赖主键 ID 的递增特性。  |  将表建立为非聚簇索引，并设置 SHARD_ROW_ID_BIT；使用应用程序发号器来定义主键 ID。 |   可以有效避免数据写入热点；可以保证数据写入性能；可以有效保证业务数据是趋势性递增 但不能保证数据连续性。   |  对原有代码有一定的改造成本；外部发号器对时钟准确性有强依赖，引入新的故障风险点。 |
|   **数据迁移推荐方案**：TIDB 未来作为主库使用，并会有大量数据写入。业务逻辑不依赖主键 ID 的连续性。   |  将表建立为聚簇索引表；主键列设置为 AUTO_RANDOM 属性。   |  可以有效避免数据写入热点；有非常有限的写入吞吐能力；主键查询性能优异；可以平滑将 AUTO_INCREMENT 属性切换为 AUTO_RANDOM 属性。    | 主键 ID 是完全随机的，业务数据排序建议使用插入时间列来完成。如果一定要使用主键 ID 排序，可以用 ID 左移 5 的方式查询，此方式查询的数据可以保证趋势递增的特性。  |
|  **数据中台推荐方案**：TIDB 未来作为只读数据库使用。  |  将表建立为非聚簇索引，并设置 SHARD_ROW_ID_BIT；使主键列维持与源数据库类型一致即可。  |  可以有效避免数据写入热点；改造成本低。   | 主键查询性能有所下降。  |

### 分库分表要点

#### 分与合

DM 支持将上游分库分表的数据合并到下游 TiDB 中的同一个表，这也是 TiDB 推荐的一种方式，合并后的收益不再赘述。这里想说另外的一个场景，就是数据归档场景。数据不断写入，随着时间流逝，大量的数据从热数据逐渐转变为温冷数据。在 TiDB 中可以通过 [Placement Rules](/configure-placement-rules.md) 放置规则来按照一定规则对数据设置不同的放置规则。而最小粒度即为表分区，即 Partition。

所以建议在遇到有大规模数据写入的场景，一开始就规划好未来是否需要归档或者有冷热数据分别存储在不同介质的需要。如果有，那么在迁移前请设置好分区表规则（目前 TiDB 还不支持 Table Rebuild 操作）。避免因为初期考虑不周，导致后期出现重新建表及数据导入这样的问题。

#### 悲观 DDL 锁与乐观 DDL 锁

在分库分表迁移与同步场景中，有时会遇到上游的分表开始进行 DDL 变更。DM 作为数据聚合工具会对此种场景进行监控，默认情况下 DM 在发现上游相关分表发生 Schema 变更后，会阻断后续的 DML 操作向下游 TiDB 写入，但会继续解析上游 Binlog。待上游各分表的 Schema 都变更完毕并自动确认所有分表的结构一致后，会从同步记录点继续同步数据。

如果你对同步效能有一定要求，可以考虑在分库分表合并场景使用乐观 DDL 锁模式。此时 DM 在发现上游分表 Schema 变更也不会阻断数据同步，而是会持续同步数据。但如果中间发现上下游数据格式不兼容的问题，将停止同步任务。此时需要人工介入处理。

自 DM v6.1.0 版本开始，默认使用乐观 DDL 锁模式。

下表汇总了乐观 DDL 锁和 悲观 DDL 锁的优劣势。

| 场景 | 推荐方案 | 优势 |
| :--- | :--- | :--- |
| 乐观 DDL 锁（默认）   | 数据同步任务基本不会出现相关阻塞延迟    | 此模式下需要保证 Schema 变更的兼容性（增加列是否具有默认值），如果考虑不周可能出现未被发现的上下游数据不一致问题。更多限制，请参考[乐观模式下分库分表合并迁移](/dm/feature-shard-merge-optimistic.md#使用限制)  |
| 悲观 DDL 锁   | 最大程度保证数据同步任务的可靠性    |  如果分表较多 ，将长时间阻断数据同步任务。并有可能因为上游的 Binlog 已被清理而导致同步中断，可以通过开启 DM-worker 的 Relay log 来避免问题。后文将更详细介绍 Relay log 的使用 |

### 其他限制与影响

#### 数据类型的影响

这里主要需要考虑上下游的数据类型问题，TiDB 目前支持绝大部分 MySQL 的数据类型。但一些特殊类型尚不支持（空间类型）。具体可以查看[数据类型概述](/data-type-overview.md)来确认是数据类型的兼容性。

#### 字符集与排序规则的影响

自 TiDB v6.0 以后，默认使用新排序规则。如果需要 TiDB 支持 utf8_general_ci、utf8mb4_general_ci、utf8_unicode_ci、utf8mb4_unicode_ci、gbk_chinese_ci 和 gbk_bin 这几种排序规则，需要在集群创建时声明。
new_collations_enabled_on_first_bootstrap 的值设为 true。更详细信息请参考[字符集和排序规则](/character-set-and-collation.md#新框架下的排序规则支持)。

TiDB 默认使用的字符集为 utf8mb4。建议同步上下游及应用统一使用 utf8mb4。如果上游有显示指定字符集或者排序规则需要确认 TiDB 是否支持。自 TiDB v6.0 版本支持 GBK 字符集。有关字符集的限制详见：

- [字符集和排序规则](/character-set-and-collation.md)
- [GBK 兼容情况](/character-set-gbk.md#与-mysql-的兼容性)

### 实施侧要点

## 数据迁移中的最佳实践

## 数据迁移后的最佳实践

## 数据长期同步的最佳实践